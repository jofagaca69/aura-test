"""
Script principal para ejecutar toda la suite de evaluaci√≥n de AURA
"""
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

# Agregar el directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.rag.vector_store import VectorStore
from src.config import config

from evaluation.test_questioner import QuestionerEvaluator, load_scenarios as load_scenarios_q
from evaluation.test_rag import RAGEvaluator, load_scenarios as load_scenarios_rag
from evaluation.test_orchestrator import OrchestratorEvaluator, load_scenarios as load_scenarios_orch
from evaluation.report_generator import ReportGenerator


class AURAEvaluationSuite:
    """
    Suite completa de evaluaci√≥n para el sistema AURA
    """
    
    def __init__(self, vector_store: VectorStore = None):
        self.vector_store = vector_store
        self.results = {}
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    def setup(self):
        """Configura el entorno de evaluaci√≥n"""
        print("\n" + "="*80)
        print("üöÄ SUITE DE EVALUACI√ìN AURA")
        print("="*80)
        print(f"üìÖ Timestamp: {self.timestamp}")
        print(f"üîß Configurando entorno...")
        
        # Validar configuraci√≥n
        try:
            config.validate()
            print("   ‚úÖ Configuraci√≥n validada")
        except Exception as e:
            print(f"   ‚ùå Error en configuraci√≥n: {e}")
            return False
        
        # Cargar VectorStore si no se proporcion√≥
        if self.vector_store is None:
            try:
                print("   üìö Cargando VectorStore...")
                self.vector_store = VectorStore()
                self.vector_store.load_vectorstore()
                print("   ‚úÖ VectorStore cargado")
            except Exception as e:
                print(f"   ‚ùå Error cargando VectorStore: {e}")
                return False
        
        return True
    
    def run_questioner_tests(self) -> bool:
        """Ejecuta tests del QuestionerAgent"""
        print("\n" + "="*80)
        print("üß™ FASE 1: Evaluaci√≥n del QuestionerAgent")
        print("="*80)
        
        try:
            scenarios = load_scenarios_q()
            evaluator = QuestionerEvaluator()
            self.results['questioner'] = evaluator.run_all_tests(scenarios)
            
            # Guardar resultados
            output_file = f"evaluation/results/questioner_{self.timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['questioner'], f, indent=2, ensure_ascii=False)
            
            print(f"\n‚úÖ Resultados guardados: {output_file}")
            return True
            
        except Exception as e:
            print(f"\n‚ùå Error en tests de QuestionerAgent: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_rag_tests(self) -> bool:
        """Ejecuta tests del sistema RAG"""
        print("\n" + "="*80)
        print("üß™ FASE 2: Evaluaci√≥n del Sistema RAG")
        print("="*80)
        
        try:
            scenarios = load_scenarios_rag()
            evaluator = RAGEvaluator(self.vector_store)
            self.results['rag'] = evaluator.run_all_tests(scenarios)
            
            # Guardar resultados
            output_file = f"evaluation/results/rag_{self.timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['rag'], f, indent=2, ensure_ascii=False)
            
            print(f"\n‚úÖ Resultados guardados: {output_file}")
            return True
            
        except Exception as e:
            print(f"\n‚ùå Error en tests de RAG: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_orchestrator_tests(self) -> bool:
        """Ejecuta tests end-to-end del Orchestrator"""
        print("\n" + "="*80)
        print("üß™ FASE 3: Evaluaci√≥n End-to-End del Orchestrator")
        print("="*80)
        
        try:
            scenarios = load_scenarios_orch()
            evaluator = OrchestratorEvaluator(self.vector_store)
            self.results['orchestrator'] = evaluator.run_all_tests(scenarios)
            
            # Guardar resultados
            output_file = f"evaluation/results/orchestrator_{self.timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['orchestrator'], f, indent=2, ensure_ascii=False)
            
            print(f"\n‚úÖ Resultados guardados: {output_file}")
            return True
            
        except Exception as e:
            print(f"\n‚ùå Error en tests de Orchestrator: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_report(self) -> str:
        """Genera reporte HTML consolidado"""
        print("\n" + "="*80)
        print("üìä Generando Reporte HTML")
        print("="*80)
        
        try:
            generator = ReportGenerator()
            output_file = f"evaluation/results/report_{self.timestamp}.html"
            
            report_path = generator.generate_full_report(
                questioner_results=self.results.get('questioner'),
                rag_results=self.results.get('rag'),
                orchestrator_results=self.results.get('orchestrator'),
                output_path=output_file
            )
            
            print(f"\n‚úÖ Reporte HTML generado: {report_path}")
            return report_path
            
        except Exception as e:
            print(f"\n‚ùå Error generando reporte: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def print_summary(self):
        """Imprime resumen final de resultados"""
        print("\n" + "="*80)
        print("üìä RESUMEN FINAL DE EVALUACI√ìN")
        print("="*80)
        
        total_components = len(self.results)
        
        print(f"\nüîç Componentes evaluados: {total_components}")
        
        for component_name, component_results in self.results.items():
            total = component_results.get('total_tests', 0)
            success = component_results.get('successful_tests', 0)
            rate = (success / total * 100) if total > 0 else 0
            
            status = "‚úÖ" if rate >= 80 else "‚ö†Ô∏è" if rate >= 60 else "‚ùå"
            
            print(f"\n{status} {component_name.upper()}:")
            print(f"   Tests: {success}/{total} ({rate:.1f}%)")
            
            # M√©tricas espec√≠ficas
            if component_name == 'questioner':
                metrics = component_results.get('metrics', {})
                print(f"   Precisi√≥n extracci√≥n: {metrics.get('avg_extraction_accuracy', 0):.1f}%")
                print(f"   Preguntas promedio: {metrics.get('avg_questions_asked', 0):.1f}")
            
            elif component_name == 'rag':
                metrics = component_results.get('metrics', {})
                print(f"   Tiempo b√∫squeda: {metrics.get('avg_search_time', 0):.3f}s")
                print(f"   Relevancia: {metrics.get('avg_relevance_score', 0):.2f}")
            
            elif component_name == 'orchestrator':
                metrics = component_results.get('metrics', {})
                exec_time = metrics.get('execution_time', {})
                llm_scores = metrics.get('llm_judge_scores', {})
                print(f"   Tiempo promedio: {exec_time.get('avg', 0):.2f}s")
                print(f"   Calidad LLM: {llm_scores.get('avg', 0):.1f}/10")
        
        # Calcular score general
        all_rates = []
        for component_results in self.results.values():
            total = component_results.get('total_tests', 0)
            success = component_results.get('successful_tests', 0)
            if total > 0:
                all_rates.append(success / total * 100)
        
        overall_rate = sum(all_rates) / len(all_rates) if all_rates else 0
        
        print(f"\n{'='*80}")
        print(f"üéØ SCORE GENERAL DEL SISTEMA: {overall_rate:.1f}%")
        
        if overall_rate >= 90:
            print("   üåü EXCELENTE - Sistema funcionando √≥ptimamente")
        elif overall_rate >= 75:
            print("   ‚úÖ BUENO - Sistema funcionando correctamente")
        elif overall_rate >= 60:
            print("   ‚ö†Ô∏è  ACEPTABLE - Algunas √°reas necesitan mejora")
        else:
            print("   ‚ùå NECESITA MEJORAS - Revisar componentes fallidos")
        
        print(f"{'='*80}\n")
    
    def run_all(self, components: list = None) -> bool:
        """
        Ejecuta toda la suite de evaluaci√≥n
        
        Args:
            components: Lista de componentes a evaluar ['questioner', 'rag', 'orchestrator']
                       Si es None, eval√∫a todos
        
        Returns:
            True si todos los tests se ejecutaron exitosamente
        """
        if components is None:
            components = ['questioner', 'rag', 'orchestrator']
        
        # Setup
        if not self.setup():
            print("\n‚ùå Error en configuraci√≥n inicial. Abortando.")
            return False
        
        success = True
        
        # Ejecutar tests seg√∫n componentes especificados
        if 'questioner' in components:
            if not self.run_questioner_tests():
                success = False
        
        if 'rag' in components:
            if not self.run_rag_tests():
                success = False
        
        if 'orchestrator' in components:
            if not self.run_orchestrator_tests():
                success = False
        
        # Generar reporte
        report_path = self.generate_report()
        
        # Imprimir resumen
        self.print_summary()
        
        # Informaci√≥n final
        print("\nüìÅ Archivos generados:")
        print(f"   - Resultados JSON: evaluation/results/*_{self.timestamp}.json")
        if report_path:
            print(f"   - Reporte HTML: {report_path}")
        
        print("\n" + "="*80)
        print("‚úÖ Evaluaci√≥n completada" if success else "‚ö†Ô∏è  Evaluaci√≥n completada con errores")
        print("="*80 + "\n")
        
        return success


def main():
    """Funci√≥n principal con argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Suite de Evaluaci√≥n AURA - Sistema Multi-Agente',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python run_evaluation.py                    # Evaluar todos los componentes
  python run_evaluation.py -c questioner      # Solo QuestionerAgent
  python run_evaluation.py -c rag orchestrator # RAG y Orchestrator
  python run_evaluation.py --quick            # Evaluaci√≥n r√°pida (menos tests)
        """
    )
    
    parser.add_argument(
        '-c', '--components',
        nargs='+',
        choices=['questioner', 'rag', 'orchestrator'],
        help='Componentes espec√≠ficos a evaluar'
    )
    
    parser.add_argument(
        '--quick',
        action='store_true',
        help='Evaluaci√≥n r√°pida (reduce n√∫mero de tests)'
    )
    
    parser.add_argument(
        '--no-report',
        action='store_true',
        help='No generar reporte HTML'
    )
    
    args = parser.parse_args()
    
    # Crear suite
    suite = AURAEvaluationSuite()
    
    # Ejecutar evaluaci√≥n
    components = args.components if args.components else None
    success = suite.run_all(components=components)
    
    # C√≥digo de salida
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

