# üöÄ Comandos √ötiles - Sistema de Evaluaci√≥n AURA

## üìã Comandos R√°pidos

### Evaluaci√≥n Completa
```bash
# Ejecutar toda la suite de evaluaci√≥n
python evaluation/run_evaluation.py

# Ver resultados
cd evaluation/results
# Abrir el archivo HTML m√°s reciente en navegador
```

### Evaluaciones Selectivas
```bash
# Solo QuestionerAgent (m√°s r√°pido, ~3 min)
python evaluation/run_evaluation.py -c questioner

# Solo RAG
python evaluation/run_evaluation.py -c rag

# Solo Orchestrator (end-to-end)
python evaluation/run_evaluation.py -c orchestrator

# RAG + Orchestrator
python evaluation/run_evaluation.py -c rag orchestrator
```

### Tests Individuales
```bash
# QuestionerAgent
python evaluation/test_questioner.py

# Sistema RAG
python evaluation/test_rag.py

# Orchestrator E2E
python evaluation/test_orchestrator.py
```

### Ejemplos Pr√°cticos
```bash
# Ver todos los ejemplos disponibles
python evaluation/examples.py

# Ejecutar ejemplo espec√≠fico
python evaluation/examples.py 1  # QuestionerAgent
python evaluation/examples.py 2  # RAG b√∫squeda custom
python evaluation/examples.py 3  # E2E custom
python evaluation/examples.py 4  # LLM Judge directo (m√°s r√°pido)
python evaluation/examples.py 5  # Comparar versiones
```

---

## üìä Ver Resultados

### Listar Resultados Recientes
```bash
# Windows PowerShell
ls evaluation/results/*.json | Sort-Object LastWriteTime -Descending | Select-Object -First 5

# Linux/Mac
ls -lt evaluation/results/*.json | head -5
```

### Abrir Reporte HTML M√°s Reciente
```bash
# Windows
start evaluation/results/report_*.html

# Linux
xdg-open evaluation/results/report_*.html

# Mac
open evaluation/results/report_*.html
```

### Ver Resumen de un Resultado JSON
```bash
# Windows PowerShell
python -c "import json; data=json.load(open('evaluation/results/orchestrator_TIMESTAMP.json')); print(f'Tasa √©xito: {data[\"success_rate\"]*100:.1f}%')"

# Linux/Mac
python3 -c "import json; data=json.load(open('evaluation/results/orchestrator_TIMESTAMP.json')); print(f'Tasa √©xito: {data[\"success_rate\"]*100:.1f}%')"
```

---

## üîß Personalizaci√≥n

### Editar Escenarios de Prueba
```bash
# Abrir en editor
code evaluation/datasets/test_scenarios.json
# o
notepad evaluation/datasets/test_scenarios.json
```

### Modificar Umbrales
```bash
# Editar configuraci√≥n
code evaluation/config.py
```

### Ajustar Prompts LLM Judge
```bash
# Editar evaluador
code evaluation/llm_judge.py
```

---

## üêç Uso desde Python

### Evaluaci√≥n Program√°tica
```python
# evaluation_script.py
from evaluation.run_evaluation import AURAEvaluationSuite
from src.rag.vector_store import VectorStore

# Cargar VectorStore
vector_store = VectorStore()
vector_store.load_vectorstore()

# Crear suite
suite = AURAEvaluationSuite(vector_store)

# Ejecutar evaluaci√≥n
suite.run_all(components=['questioner', 'rag'])

# Acceder a resultados
print(suite.results)
```

### Test Espec√≠fico
```python
from evaluation.test_questioner import QuestionerEvaluator, load_scenarios

scenarios = load_scenarios()
evaluator = QuestionerEvaluator()

# Test un escenario
result = evaluator.test_scenario(scenarios[0])
print(f"√âxito: {result['overall_success']}")
```

### LLM Judge Directo
```python
from evaluation.llm_judge import LLMJudge

judge = LLMJudge()
result = judge.evaluate_recommendations(
    user_analysis="Usuario busca laptop para gaming",
    search_criteria="Presupuesto: 1500‚Ç¨, GPU potente",
    recommendations="1. ASUS ROG...\n2. MSI...",
    products_found=2
)

if result['success']:
    print(f"Score: {result['evaluation']['score_total']}/10")
```

---

## üìà An√°lisis de Resultados

### Comparar Dos Evaluaciones
```python
import json

# Cargar resultados
with open('evaluation/results/orchestrator_old.json') as f:
    old = json.load(f)
with open('evaluation/results/orchestrator_new.json') as f:
    new = json.load(f)

# Comparar
print(f"Tasa √©xito:")
print(f"  Antes: {old['success_rate']*100:.1f}%")
print(f"  Ahora: {new['success_rate']*100:.1f}%")
print(f"  Mejora: {(new['success_rate']-old['success_rate'])*100:+.1f}%")
```

### Extraer M√©tricas Espec√≠ficas
```python
import json
from pathlib import Path

# Buscar todos los resultados de orchestrator
results_dir = Path('evaluation/results')
files = sorted(results_dir.glob('orchestrator_*.json'))

# Extraer scores LLM Judge
llm_scores = []
for file in files:
    with open(file) as f:
        data = json.load(f)
        score = data['metrics']['llm_judge_scores']['avg']
        llm_scores.append((file.stem, score))

# Imprimir evoluci√≥n
for name, score in llm_scores:
    print(f"{name}: {score:.1f}/10")
```

---

## üîÑ Automatizaci√≥n

### Script de Evaluaci√≥n Peri√≥dica
```bash
#!/bin/bash
# evaluate_daily.sh

cd /path/to/aura-test/aura-test
python evaluation/run_evaluation.py

# Enviar notificaci√≥n (opcional)
echo "Evaluaci√≥n completada" | mail -s "AURA Evaluation" you@email.com
```

### Cron Job (Linux/Mac)
```bash
# Ejecutar evaluaci√≥n diaria a las 2 AM
0 2 * * * cd /path/to/aura && python evaluation/run_evaluation.py >> /var/log/aura_eval.log 2>&1
```

### Task Scheduler (Windows)
```powershell
# Crear tarea programada
$action = New-ScheduledTaskAction -Execute "python" -Argument "evaluation/run_evaluation.py" -WorkingDirectory "C:\path\to\aura"
$trigger = New-ScheduledTaskTrigger -Daily -At 2am
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "AURA Evaluation"
```

---

## üêõ Debug y Troubleshooting

### Modo Verbose
```python
# Agregar prints detallados en los evaluadores
# Editar test_*.py y descomentar prints de debug
```

### Ver Trazas Completas
```python
# Ejecutar con traceback completo
import traceback
try:
    evaluator.run_all_tests(scenarios)
except Exception as e:
    traceback.print_exc()
```

### Test con un Solo Escenario
```python
from evaluation.test_orchestrator import OrchestratorEvaluator
from src.rag.vector_store import VectorStore

vector_store = VectorStore()
vector_store.load_vectorstore()

evaluator = OrchestratorEvaluator(vector_store)

# Crear escenario m√≠nimo para debug
test_scenario = {
    "id": "debug_test",
    "name": "Debug test",
    "category": "laptop",
    "difficulty": "easy",
    "conversation": ["Busco laptop", "Para trabajo", "800 euros"],
    "expected_outcome": {"should_find_products": True, "min_products": 1}
}

result = evaluator.test_scenario(test_scenario)
print(result)
```

---

## üì¶ Exportar/Importar Resultados

### Exportar a CSV
```python
import json
import csv

# Cargar JSON
with open('evaluation/results/orchestrator_20250111.json') as f:
    data = json.load(f)

# Exportar m√©tricas a CSV
with open('metrics.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['Escenario', '√âxito', 'Tiempo', 'Productos'])
    
    for result in data['detailed_results']:
        writer.writerow([
            result['scenario_name'],
            result['overall_success'],
            result['execution_time'],
            result['metrics']['products_found']
        ])
```

### Generar Reporte desde M√∫ltiples Archivos
```bash
python evaluation/report_generator.py \
  evaluation/results/questioner_20250111_120000.json \
  evaluation/results/rag_20250111_120000.json \
  evaluation/results/orchestrator_20250111_120000.json
```

---

## üéØ Tips de Performance

### Reducir Tiempo de Evaluaci√≥n
```python
# Opci√≥n 1: Evaluar menos escenarios
# Editar test_scenarios.json y comentar algunos

# Opci√≥n 2: Solo componentes r√°pidos
python evaluation/run_evaluation.py -c questioner

# Opci√≥n 3: Modificar K en RAG (menos resultados)
# En test_rag.py, cambiar k=10 a k=5
```

### Usar Gemini Flash (M√°s R√°pido/Barato)
```python
# En llm_judge.py, cambiar modelo
LLM_JUDGE_CONFIG = {
    "model": "gemini-1.5-flash",  # En vez de gemini-1.5-pro
    "temperature": 0.1
}
```

---

## üìö Recursos Adicionales

### Documentaci√≥n
```bash
# Leer documentaci√≥n completa
cat evaluation/README.md

# Gu√≠a r√°pida
cat evaluation/QUICKSTART.md

# Resumen de implementaci√≥n
cat evaluation/IMPLEMENTATION_SUMMARY.md
```

### Ver Ejemplos
```bash
# Ver c√≥digo de ejemplos
cat evaluation/examples.py

# Ejecutar todos los ejemplos
python evaluation/examples.py
```

---

## üí° Recetas √ötiles

### 1. Evaluaci√≥n R√°pida Pre-Commit
```bash
# Antes de hacer commit, ejecutar evaluaci√≥n r√°pida
python evaluation/run_evaluation.py -c questioner
```

### 2. Benchmark de Rendimiento
```bash
# Medir tiempo de cada componente
time python evaluation/test_questioner.py
time python evaluation/test_rag.py
time python evaluation/test_orchestrator.py
```

### 3. Generar Reporte Comparativo
```python
# compare_reports.py
from evaluation.report_generator import ReportGenerator
import json

# Cargar dos versiones
old_data = json.load(open('results/old/orchestrator.json'))
new_data = json.load(open('results/new/orchestrator.json'))

# Generar reporte comparativo
generator = ReportGenerator()
# Implementar l√≥gica de comparaci√≥n...
```

---

## üéì Mejores Pr√°cticas

1. **Ejecutar evaluaci√≥n antes de cada release**
   ```bash
   python evaluation/run_evaluation.py
   ```

2. **Mantener historial de resultados**
   ```bash
   # No borrar archivos de results/, son tu historial
   ```

3. **Revisar tests fallidos individualmente**
   ```python
   # Ver detalles de tests fallidos en el JSON
   ```

4. **Ajustar umbrales seg√∫n tu caso de uso**
   ```python
   # Editar config.py seg√∫n necesidades
   ```

5. **Documentar cambios en escenarios**
   ```json
   // Agregar comentarios en test_scenarios.json
   ```

---

**¬°Listo para evaluar! üöÄ**

Para m√°s informaci√≥n, consulta:
- `README.md` - Documentaci√≥n completa
- `QUICKSTART.md` - Inicio r√°pido
- `examples.py` - Ejemplos pr√°cticos

