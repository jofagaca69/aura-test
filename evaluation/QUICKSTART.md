# ğŸš€ GuÃ­a RÃ¡pida - EvaluaciÃ³n AURA

## âš¡ Inicio RÃ¡pido (5 minutos)

### 1ï¸âƒ£ PreparaciÃ³n

```bash
# 1. AsegÃºrate de tener el VectorStore procesado
#    (desde Streamlit -> ConfiguraciÃ³n -> Procesar Documentos)

# 2. Verifica que .env tiene GOOGLE_API_KEY configurada
```

### 2ï¸âƒ£ Ejecutar EvaluaciÃ³n Completa

```bash
cd evaluation
python run_evaluation.py
```

Esto ejecutarÃ¡:
- âœ… Tests del QuestionerAgent (8 escenarios)
- âœ… Tests del Sistema RAG (8 escenarios)  
- âœ… Tests End-to-End (8 escenarios)
- âœ… GeneraciÃ³n de reporte HTML

**Tiempo estimado:** 10-15 minutos

### 3ï¸âƒ£ Ver Resultados

```bash
# Abrir el reporte HTML generado
# Se guarda en: evaluation/results/report_TIMESTAMP.html
```

El reporte incluye:
- ğŸ“Š MÃ©tricas generales del sistema
- ğŸ” AnÃ¡lisis detallado por componente
- ğŸ¯ Scores de calidad LLM Judge
- âŒ Tests fallidos con detalles

---

## ğŸ¯ Casos de Uso Comunes

### Solo Evaluar un Componente

```bash
# Solo QuestionerAgent (mÃ¡s rÃ¡pido)
python run_evaluation.py -c questioner

# Solo RAG
python run_evaluation.py -c rag

# Solo Orchestrator (end-to-end)
python run_evaluation.py -c orchestrator
```

### Tests Individuales

```bash
# Test especÃ­fico del QuestionerAgent
python test_questioner.py

# Test especÃ­fico del RAG
python test_rag.py

# Test especÃ­fico del Orchestrator
python test_orchestrator.py
```

### Generar Reporte desde JSON Existentes

```bash
python report_generator.py \
  results/questioner_20250111_120000.json \
  results/rag_20250111_120000.json \
  results/orchestrator_20250111_120000.json
```

---

## ğŸ“Š InterpretaciÃ³n RÃ¡pida

### Scores del Sistema

| Score | Estado | Significado |
|-------|--------|-------------|
| â‰¥ 90% | ğŸŒŸ EXCELENTE | Todo funciona perfectamente |
| 75-89% | âœ… BUENO | Sistema funcional, pequeÃ±as mejoras posibles |
| 60-74% | âš ï¸ ACEPTABLE | Funciona pero necesita optimizaciÃ³n |
| < 60% | âŒ DEFICIENTE | Requiere atenciÃ³n urgente |

### MÃ©tricas Clave por Componente

**QuestionerAgent:**
- âœ… PrecisiÃ³n extracciÃ³n > 85%
- âœ… Preguntas â‰¤ 5
- âœ… Score informaciÃ³n > 60%

**RAG:**
- âœ… Tiempo bÃºsqueda < 2s
- âœ… Relevancia > 70%
- âœ… VectorStore HEALTHY

**Orchestrator:**
- âœ… Tasa Ã©xito > 90%
- âœ… Tiempo total < 30s
- âœ… Calidad LLM > 8/10

---

## ğŸ” SoluciÃ³n de Problemas

### Error: VectorStore no encontrado

```bash
# SoluciÃ³n: Procesar documentos primero
# 1. Abre Streamlit: streamlit run app.py
# 2. Ve a ConfiguraciÃ³n -> InicializaciÃ³n RAG
# 3. Haz clic en "Procesar Documentos"
```

### Error: GOOGLE_API_KEY no configurada

```bash
# SoluciÃ³n: Crear archivo .env
echo "GOOGLE_API_KEY=tu_api_key_aqui" > .env
```

### Tests muy lentos

```bash
# Reducir nÃºmero de escenarios en test_scenarios.json
# O ejecutar componentes individualmente
python run_evaluation.py -c questioner  # MÃ¡s rÃ¡pido
```

### Error de importaciÃ³n

```bash
# AsegÃºrate de ejecutar desde el directorio correcto
cd evaluation
python run_evaluation.py

# Si persiste, verifica que estÃ¡s en el entorno correcto
# Con uv:
uv sync
```

---

## ğŸ“ Archivos Generados

DespuÃ©s de la evaluaciÃ³n, encontrarÃ¡s:

```
evaluation/results/
â”œâ”€â”€ questioner_TIMESTAMP.json      # Resultados QuestionerAgent
â”œâ”€â”€ rag_TIMESTAMP.json             # Resultados RAG
â”œâ”€â”€ orchestrator_TIMESTAMP.json    # Resultados Orchestrator
â””â”€â”€ report_TIMESTAMP.html          # Reporte visual
```

**RecomendaciÃ³n:** Abre el archivo HTML para anÃ¡lisis visual completo.

---

## ğŸ’¡ Tips Pro

### 1. Automatizar Evaluaciones

```bash
# Crear un cron job para evaluaciones periÃ³dicas
0 2 * * * cd /path/to/aura && python evaluation/run_evaluation.py
```

### 2. Comparar Resultados

```bash
# Guarda los JSON de diferentes fechas y compara mÃ©tricas
python -c "
import json
with open('results/orchestrator_old.json') as f:
    old = json.load(f)
with open('results/orchestrator_new.json') as f:
    new = json.load(f)
print(f'Mejora: {new[\"success_rate\"] - old[\"success_rate\"]:.2%}')
"
```

### 3. CI/CD Integration

```yaml
# .github/workflows/evaluate.yml
name: AURA Evaluation
on: [push]
jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run evaluation
        run: python evaluation/run_evaluation.py
```

### 4. Custom Scenarios

```json
// Crea tus propios tests en datasets/test_scenarios.json
{
  "id": "my_test",
  "name": "Mi caso especÃ­fico",
  "conversation": ["..."],
  "expected_extraction": {...}
}
```

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… Ejecuta evaluaciÃ³n completa
2. ğŸ“Š Revisa reporte HTML
3. ğŸ” Identifica Ã¡reas de mejora
4. ğŸ› ï¸ Optimiza componentes dÃ©biles
5. ğŸ”„ Re-evalÃºa para confirmar mejoras
6. ğŸ“ˆ Trackea mÃ©tricas en el tiempo

---

## ğŸ“ Soporte

Â¿Problemas? Revisa:
- ğŸ“– [README completo](./README.md)
- ğŸ”§ [ConfiguraciÃ³n](./config.py)
- ğŸ“Š Logs en consola durante ejecuciÃ³n

---

**Â¡Listo para evaluar tu sistema AURA! ğŸš€**

