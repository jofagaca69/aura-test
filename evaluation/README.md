# ğŸ§ª Sistema de EvaluaciÃ³n AURA

Sistema completo de evaluaciÃ³n para el sistema multi-agente AURA de recomendaciÃ³n de productos.

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n General](#descripciÃ³n-general)
- [Arquitectura](#arquitectura)
- [Componentes Evaluados](#componentes-evaluados)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Uso](#uso)
- [MÃ©tricas y Umbrales](#mÃ©tricas-y-umbrales)
- [InterpretaciÃ³n de Resultados](#interpretaciÃ³n-de-resultados)
- [Ejemplos](#ejemplos)

## ğŸ¯ DescripciÃ³n General

Este sistema de evaluaciÃ³n proporciona una suite completa para medir y validar el rendimiento de AURA a travÃ©s de mÃºltiples dimensiones:

- âœ… **PrecisiÃ³n de extracciÃ³n de informaciÃ³n** (QuestionerAgent)
- âœ… **Calidad de preguntas contextuales** (LLM-as-Judge)
- âœ… **Relevancia de bÃºsqueda RAG** (VectorStore)
- âœ… **Calidad de recomendaciones** (LLM-as-Judge)
- âœ… **Rendimiento end-to-end** (Orchestrator)

## ğŸ—ï¸ Arquitectura

```
evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                    # ConfiguraciÃ³n y umbrales
â”œâ”€â”€ llm_judge.py                # Evaluador LLM-as-Judge
â”œâ”€â”€ test_questioner.py          # Tests del QuestionerAgent
â”œâ”€â”€ test_rag.py                 # Tests del sistema RAG
â”œâ”€â”€ test_orchestrator.py        # Tests end-to-end
â”œâ”€â”€ report_generator.py         # Generador de reportes HTML
â”œâ”€â”€ run_evaluation.py           # Script principal
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ test_scenarios.json    # Escenarios de prueba
â””â”€â”€ results/                    # Resultados generados
    â”œâ”€â”€ *.json                 # Resultados en JSON
    â””â”€â”€ *.html                 # Reportes HTML
```

## ğŸ” Componentes Evaluados

### 1. QuestionerAgent

**EvalÃºa:**
- PrecisiÃ³n de extracciÃ³n de informaciÃ³n estructurada
- Calidad y contextualidad de preguntas generadas
- Eficiencia (nÃºmero de preguntas vs informaciÃ³n obtenida)
- Score de completitud de informaciÃ³n

**MÃ©tricas clave:**
```python
{
  "extraction_accuracy": 85%,      # MÃ­nimo esperado
  "max_questions": 5,              # MÃ¡ximo permitido
  "information_score_min": 60%     # Score mÃ­nimo de info
}
```

### 2. Sistema RAG

**EvalÃºa:**
- Velocidad de bÃºsqueda en VectorStore
- Relevancia de documentos recuperados
- Salud del sistema (conectividad, diversidad)
- Precision@K y otras mÃ©tricas de recuperaciÃ³n

**MÃ©tricas clave:**
```python
{
  "search_time_max": 2.0,          # MÃ¡ximo 2 segundos
  "precision_at_5": 80%,           # Top 5 resultados
  "recall_at_10": 70%              # Top 10 resultados
}
```

### 3. Orchestrator (End-to-End)

**EvalÃºa:**
- Flujo completo de conversaciÃ³n
- Calidad de recomendaciones finales (LLM Judge)
- Tiempo total de ejecuciÃ³n
- Productos encontrados vs esperados
- Relevancia de recomendaciones

**MÃ©tricas clave:**
```python
{
  "success_rate": 90%,             # Tasa de Ã©xito
  "avg_time_max": 30.0,           # MÃ¡ximo 30 segundos
  "products_found_min": 1,         # Al menos 1 producto
  "llm_judge_min_score": 8.0      # Calidad mÃ­nima 8/10
}
```

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

```bash
# Instalar dependencias (si usas uv)
uv sync

# O con pip
pip install langchain langchain-google-genai chromadb sentence-transformers
```

### ConfiguraciÃ³n

1. **Configurar variables de entorno:**
```bash
# .env
GOOGLE_API_KEY=your_key_here
```

2. **Verificar VectorStore:**
```bash
# AsegÃºrate de que el VectorStore estÃ¡ procesado
# Ve a la pÃ¡gina de ConfiguraciÃ³n en Streamlit para procesar documentos
```

3. **Verificar escenarios de prueba:**
```bash
# Los escenarios estÃ¡n en evaluation/datasets/test_scenarios.json
# Puedes personalizarlos segÃºn tus necesidades
```

## ğŸ“– Uso

### EvaluaciÃ³n Completa

Ejecutar todos los componentes:

```bash
python evaluation/run_evaluation.py
```

### EvaluaciÃ³n Selectiva

Evaluar componentes especÃ­ficos:

```bash
# Solo QuestionerAgent
python evaluation/run_evaluation.py -c questioner

# Solo RAG
python evaluation/run_evaluation.py -c rag

# RAG + Orchestrator
python evaluation/run_evaluation.py -c rag orchestrator
```

### EvaluaciÃ³n Individual

Ejecutar tests de forma independiente:

```bash
# QuestionerAgent
python evaluation/test_questioner.py

# RAG
python evaluation/test_rag.py

# Orchestrator
python evaluation/test_orchestrator.py
```

### Generar Reporte desde Resultados Existentes

```bash
python evaluation/report_generator.py \
  evaluation/results/questioner_20250101_120000.json \
  evaluation/results/rag_20250101_120000.json \
  evaluation/results/orchestrator_20250101_120000.json
```

## ğŸ“Š MÃ©tricas y Umbrales

### ConfiguraciÃ³n de Umbrales

Edita `evaluation/config.py` para ajustar umbrales:

```python
THRESHOLDS = {
    "questioner": {
        "extraction_accuracy": 0.85,     # 85%
        "max_questions": 5,
        "information_score_min": 60.0
    },
    "rag": {
        "precision_at_5": 0.80,         # 80%
        "search_time_max": 2.0          # 2 segundos
    },
    "recommender": {
        "llm_judge_min_score": 8.0,     # 8/10
        "relevancia_min": 7.0,
        "diversidad_min": 6.0
    },
    "orchestrator": {
        "success_rate": 0.90,            # 90%
        "avg_time_max": 30.0,
        "products_found_min": 1
    }
}
```

### LLM-as-Judge

El evaluador LLM usa Gemini para juzgar calidad con 5 criterios:

1. **Relevancia (0-10):** Â¿Los productos coinciden con necesidades?
2. **Diversidad (0-10):** Â¿Hay variedad apropiada?
3. **ExplicaciÃ³n (0-10):** Â¿Las justificaciones son claras?
4. **PersonalizaciÃ³n (0-10):** Â¿Se adaptÃ³ al contexto?
5. **Completitud (0-10):** Â¿Se abordaron todos los criterios?

**Score total:** Promedio de los 5 criterios

## ğŸ“ˆ InterpretaciÃ³n de Resultados

### Formato de Resultados

Los resultados se guardan en dos formatos:

1. **JSON** (`evaluation/results/*.json`): Datos estructurados detallados
2. **HTML** (`evaluation/results/report_*.html`): Reporte visual interactivo

### Estructura de Resultados JSON

```json
{
  "component": "QuestionerAgent",
  "timestamp": "2025-01-01T12:00:00",
  "total_tests": 8,
  "successful_tests": 7,
  "success_rate": 0.875,
  "metrics": {
    "avg_extraction_accuracy": 87.5,
    "avg_questions_asked": 4.2,
    "avg_information_score": 75.3
  },
  "thresholds": { ... },
  "all_thresholds_passed": {
    "extraction_accuracy": true,
    "max_questions": true
  },
  "detailed_results": [ ... ]
}
```

### InterpretaciÃ³n de Scores

| Score | InterpretaciÃ³n | AcciÃ³n |
|-------|---------------|--------|
| â‰¥ 90% | ğŸŒŸ EXCELENTE | Mantener calidad |
| 75-89% | âœ… BUENO | PequeÃ±as optimizaciones |
| 60-74% | âš ï¸ ACEPTABLE | Revisar Ã¡reas dÃ©biles |
| < 60% | âŒ DEFICIENTE | Requiere mejoras urgentes |

### Reporte HTML

Abre el archivo HTML generado en un navegador para ver:

- ğŸ“Š **Resumen ejecutivo** con mÃ©tricas generales
- ğŸ” **Detalle por componente** con tablas y grÃ¡ficos
- ğŸ“ˆ **MÃ©tricas de rendimiento** por escenario
- ğŸ¯ **AnÃ¡lisis de calidad LLM Judge**
- âš ï¸ **Tests fallidos** con detalles de error

## ğŸ’¡ Ejemplos

### Ejemplo 1: EvaluaciÃ³n Completa

```bash
$ python evaluation/run_evaluation.py

================================================================================
ğŸš€ SUITE DE EVALUACIÃ“N AURA
================================================================================
ğŸ“… Timestamp: 20250111_143022
ğŸ”§ Configurando entorno...
   âœ… ConfiguraciÃ³n validada
   ğŸ“š Cargando VectorStore...
   âœ… VectorStore cargado

================================================================================
ğŸ§ª FASE 1: EvaluaciÃ³n del QuestionerAgent
================================================================================
...
Tests ejecutados: 8
Tests exitosos: 7 (87.5%)

================================================================================
ğŸ§ª FASE 2: EvaluaciÃ³n del Sistema RAG
================================================================================
...
Tests ejecutados: 8
Tests exitosos: 7 (87.5%)

================================================================================
ğŸ§ª FASE 3: EvaluaciÃ³n End-to-End del Orchestrator
================================================================================
...
Tests ejecutados: 8
Tests exitosos: 6 (75.0%)

================================================================================
ğŸ“Š RESUMEN FINAL DE EVALUACIÃ“N
================================================================================
ğŸ¯ SCORE GENERAL DEL SISTEMA: 83.3%
   âœ… BUENO - Sistema funcionando correctamente

ğŸ“ Archivos generados:
   - Resultados JSON: evaluation/results/*_20250111_143022.json
   - Reporte HTML: evaluation/results/report_20250111_143022.html
```

### Ejemplo 2: Test Unitario de QuestionerAgent

```python
from evaluation.test_questioner import QuestionerEvaluator, load_scenarios

# Cargar escenarios
scenarios = load_scenarios()

# Crear evaluador
evaluator = QuestionerEvaluator()

# Ejecutar un escenario especÃ­fico
result = evaluator.test_scenario(scenarios[0])

print(f"PrecisiÃ³n: {result['extraction_evaluation']['accuracy_percentage']:.1f}%")
print(f"Preguntas: {result['questions_asked']}")
print(f"Ã‰xito: {result['overall_success']}")
```

### Ejemplo 3: EvaluaciÃ³n Custom con Escenarios Propios

```python
import json
from evaluation.test_orchestrator import OrchestratorEvaluator
from src.rag.vector_store import VectorStore

# Cargar VectorStore
vector_store = VectorStore()
vector_store.load_vectorstore()

# Crear evaluador
evaluator = OrchestratorEvaluator(vector_store)

# Definir escenario custom
custom_scenario = {
    "id": "custom_1",
    "name": "Test personalizado",
    "category": "laptop",
    "difficulty": "medium",
    "conversation": [
        "Busco una laptop",
        "Para trabajo de oficina",
        "Hasta 800 euros",
        "Que sea ligera",
        "Sin preferencias de marca"
    ],
    "expected_extraction": {
        "categoria_producto": "laptop",
        "presupuesto_max": 800.0,
        "uso_principal": "trabajo de oficina",
        "caracteristicas_clave": ["ligera"]
    },
    "expected_outcome": {
        "should_find_products": True,
        "min_products": 1,
        "max_questions": 5,
        "relevance_keywords": ["laptop", "oficina", "ligera"]
    }
}

# Ejecutar test
result = evaluator.test_scenario(custom_scenario)

# Ver resultados
print(json.dumps(result, indent=2, ensure_ascii=False))
```

## ğŸ”§ PersonalizaciÃ³n

### AÃ±adir Nuevos Escenarios de Prueba

Edita `evaluation/datasets/test_scenarios.json`:

```json
{
  "scenarios": [
    {
      "id": "my_scenario",
      "name": "Mi escenario personalizado",
      "category": "producto",
      "difficulty": "medium",
      "conversation": [
        "Primera respuesta del usuario",
        "Segunda respuesta",
        ...
      ],
      "expected_extraction": {
        "categoria_producto": "laptop",
        "presupuesto_max": 1000.0,
        ...
      },
      "expected_outcome": {
        "should_find_products": true,
        "min_products": 1,
        "relevance_keywords": ["keyword1", "keyword2"]
      }
    }
  ]
}
```

### Modificar Prompts de LLM Judge

Edita los prompts en `evaluation/llm_judge.py` para ajustar criterios de evaluaciÃ³n.

### Crear Evaluadores Custom

```python
from evaluation.llm_judge import LLMJudge
from src.agents.base_agent import BaseAgent

class MyCustomEvaluator:
    def __init__(self):
        self.llm_judge = LLMJudge()
    
    def evaluate_custom_metric(self, data):
        # Tu lÃ³gica de evaluaciÃ³n
        pass
```

## ğŸ“š Referencias

- **LangChain Documentation:** https://python.langchain.com/
- **Google Gemini:** https://ai.google.dev/
- **LangSmith (Tracing):** https://smith.langchain.com/
- **ChromaDB:** https://www.trychroma.com/

## ğŸ¤ Contribuciones

Para aÃ±adir nuevas evaluaciones o mejorar las existentes:

1. Crea nuevos escenarios de prueba en `datasets/`
2. AÃ±ade evaluadores especÃ­ficos siguiendo el patrÃ³n existente
3. Actualiza umbrales en `config.py` segÃºn necesidad
4. Documenta nuevas mÃ©tricas en este README

## ğŸ“ Notas Importantes

âš ï¸ **Consideraciones:**

- Los tests con LLM Judge pueden tardar varios minutos
- Los costos de API aumentan con mÃ¡s tests (usa Gemini Flash para reducir costos)
- Los resultados pueden variar ligeramente entre ejecuciones debido a la naturaleza no determinÃ­stica de los LLMs
- AsegÃºrate de tener suficientes datos en el VectorStore antes de evaluar

ğŸ¯ **Mejores PrÃ¡cticas:**

- Ejecuta evaluaciones en entorno de desarrollo antes de producciÃ³n
- MantÃ©n un historial de resultados para tracking de mejoras
- Revisa tests fallidos individualmente para entender causas raÃ­z
- Ajusta umbrales segÃºn tu caso de uso especÃ­fico

---

**Creado por:** JosÃ© FabiÃ¡n GarcÃ­a Camargo
**VersiÃ³n:** 1.0
**Fecha:** Noviembre 2025

