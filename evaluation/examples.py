"""
Ejemplos pr√°cticos de uso del sistema de evaluaci√≥n AURA
"""
import sys
from pathlib import Path

# Agregar directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.rag.vector_store import VectorStore
from evaluation.test_questioner import QuestionerEvaluator, load_scenarios
from evaluation.test_rag import RAGEvaluator
from evaluation.test_orchestrator import OrchestratorEvaluator
from evaluation.llm_judge import LLMJudge


# ============================================================================
# EJEMPLO 1: Evaluar un escenario espec√≠fico del QuestionerAgent
# ============================================================================

def example_1_questioner_single_scenario():
    """
    Eval√∫a un solo escenario del QuestionerAgent
    √ötil para debug o an√°lisis detallado
    """
    print("\n" + "="*80)
    print("EJEMPLO 1: Test Individual - QuestionerAgent")
    print("="*80)
    
    # Cargar escenarios
    scenarios = load_scenarios()
    
    # Seleccionar uno espec√≠fico (por ejemplo, el primero)
    scenario = scenarios[0]
    
    # Crear evaluador
    evaluator = QuestionerEvaluator()
    
    # Ejecutar test
    result = evaluator.test_scenario(scenario)
    
    # Analizar resultados
    print("\nüìä An√°lisis del resultado:")
    print(f"  Escenario: {result['scenario_name']}")
    print(f"  √âxito: {'‚úÖ' if result['overall_success'] else '‚ùå'}")
    print(f"  Precisi√≥n extracci√≥n: {result['extraction_evaluation']['accuracy_percentage']:.1f}%")
    print(f"  Preguntas realizadas: {result['questions_asked']}")
    print(f"  Score informaci√≥n: {result['information_score']:.1f}%")
    
    # Ver preguntas generadas
    print("\n‚ùì Preguntas generadas:")
    for i, q in enumerate(result['questions_list'], 1):
        print(f"  {i}. {q}")
    
    return result


# ============================================================================
# EJEMPLO 2: Evaluar b√∫squeda RAG con consulta custom
# ============================================================================

def example_2_rag_custom_query():
    """
    Prueba el sistema RAG con una consulta personalizada
    √ötil para verificar recuperaci√≥n de documentos
    """
    print("\n" + "="*80)
    print("EJEMPLO 2: B√∫squeda RAG Custom")
    print("="*80)
    
    # Cargar VectorStore
    print("üöÄ Cargando VectorStore...")
    vector_store = VectorStore()
    vector_store.load_vectorstore()
    
    # Crear evaluador
    evaluator = RAGEvaluator(vector_store)
    
    # Definir consultas de prueba
    test_queries = [
        {
            "query": "laptop para programaci√≥n con 16GB RAM",
            "expected_keywords": ["laptop", "programaci√≥n", "RAM", "16GB"]
        },
        {
            "query": "tel√©fono econ√≥mico con buena bater√≠a",
            "expected_keywords": ["tel√©fono", "econ√≥mico", "bater√≠a"]
        },
        {
            "query": "auriculares con cancelaci√≥n de ruido",
            "expected_keywords": ["auriculares", "cancelaci√≥n", "ruido"]
        }
    ]
    
    # Ejecutar b√∫squedas
    for test in test_queries:
        result = evaluator.test_retrieval(
            query=test["query"],
            expected_keywords=test["expected_keywords"],
            k=5
        )
        
        print(f"\nüìä Resultados para: '{test['query']}'")
        print(f"  Documentos encontrados: {result['num_results']}")
        print(f"  Tiempo: {result['search_time']:.3f}s")
        print(f"  Relevancia: {result['relevance_score']:.2%}")
        
        if result['results_preview']:
            print(f"\n  üìÑ Preview del primer resultado:")
            print(f"  {result['results_preview'][0]['content'][:150]}...")


# ============================================================================
# EJEMPLO 3: Evaluar flujo end-to-end con escenario custom
# ============================================================================

def example_3_e2e_custom_scenario():
    """
    Ejecuta un test end-to-end con un escenario personalizado
    √ötil para probar casos de uso espec√≠ficos
    """
    print("\n" + "="*80)
    print("EJEMPLO 3: Test E2E con Escenario Custom")
    print("="*80)
    
    # Cargar VectorStore
    print("üöÄ Cargando VectorStore...")
    vector_store = VectorStore()
    vector_store.load_vectorstore()
    
    # Crear evaluador
    evaluator = OrchestratorEvaluator(vector_store)
    
    # Definir escenario custom
    custom_scenario = {
        "id": "custom_example",
        "name": "Test: Usuario busca laptop para dise√±o gr√°fico",
        "category": "laptop",
        "difficulty": "medium",
        "conversation": [
            "Necesito una laptop nueva",
            "La uso principalmente para dise√±o gr√°fico con Photoshop e Illustrator",
            "Mi presupuesto es de unos 1500 euros",
            "Necesito buena pantalla y al menos 16GB de RAM",
            "Prefiero marcas confiables como Apple o Dell"
        ],
        "expected_extraction": {
            "categoria_producto": "laptop",
            "presupuesto_max": 1500.0,
            "uso_principal": "dise√±o gr√°fico",
            "caracteristicas_clave": ["buena pantalla", "16GB RAM"],
            "preferencias_marca": ["Apple", "Dell"]
        },
        "expected_outcome": {
            "should_find_products": True,
            "min_products": 1,
            "max_questions": 5,
            "relevance_keywords": ["dise√±o", "gr√°fico", "pantalla", "RAM"]
        }
    }
    
    # Ejecutar test
    result = evaluator.test_scenario(custom_scenario)
    
    # Analizar resultados
    print("\nüìä An√°lisis del resultado:")
    print(f"  Estado final: {result['final_state']}")
    print(f"  Productos encontrados: {result['workflow_data']['products_found']}")
    print(f"  Tiempo ejecuci√≥n: {result['execution_time']:.2f}s")
    print(f"  √âxito: {'‚úÖ' if result['overall_success'] else '‚ùå'}")
    
    # Ver recomendaciones generadas
    if result['workflow_data']['recommendations_preview']:
        print("\nüéØ Preview de recomendaciones:")
        print(result['workflow_data']['recommendations_preview'])
    
    # Ver calidad seg√∫n LLM Judge
    if result['evaluations']['recommendations_quality'].get('success'):
        eval_data = result['evaluations']['recommendations_quality']['evaluation']
        print("\n‚≠ê Evaluaci√≥n de calidad (LLM Judge):")
        print(f"  Relevancia: {eval_data.get('relevancia', 'N/A'):.1f}/10")
        print(f"  Diversidad: {eval_data.get('diversidad', 'N/A'):.1f}/10")
        print(f"  Explicaci√≥n: {eval_data.get('explicacion', 'N/A'):.1f}/10")
        print(f"  Personalizaci√≥n: {eval_data.get('personalizacion', 'N/A'):.1f}/10")
        print(f"  Completitud: {eval_data.get('completitud', 'N/A'):.1f}/10")
        print(f"  Score Total: {eval_data.get('score_total', 'N/A'):.1f}/10")
    
    return result


# ============================================================================
# EJEMPLO 4: Usar LLM Judge directamente
# ============================================================================

def example_4_llm_judge_direct():
    """
    Usa el LLM Judge directamente para evaluar recomendaciones
    √ötil para evaluar outputs de forma independiente
    """
    print("\n" + "="*80)
    print("EJEMPLO 4: Uso Directo de LLM Judge")
    print("="*80)
    
    # Crear judge
    judge = LLMJudge()
    
    # Simular datos de entrada
    user_analysis = """
    El usuario busca una laptop para dise√±o gr√°fico.
    Presupuesto: 1500 euros
    Uso principal: Photoshop, Illustrator
    Caracter√≠sticas importantes: Buena pantalla, 16GB RAM
    Marcas preferidas: Apple, Dell
    """
    
    search_criteria = """
    Categor√≠a: Laptops
    Precio: hasta 1500‚Ç¨
    RAM: m√≠nimo 16GB
    Pantalla: alta calidad de color
    Uso: dise√±o gr√°fico profesional
    """
    
    recommendations = """
    He encontrado 3 opciones excelentes para ti:
    
    1. **MacBook Pro 14" M3** - 1499‚Ç¨
       - Pantalla Retina de alta calidad
       - 16GB RAM unificada
       - Excelente para dise√±o con Adobe Suite
       - Bater√≠a de larga duraci√≥n
    
    2. **Dell XPS 15** - 1399‚Ç¨
       - Pantalla 4K OLED
       - 16GB RAM DDR5
       - NVIDIA RTX 3050
       - Gran rendimiento en dise√±o
    
    3. **HP Envy 16** - 1299‚Ç¨
       - Pantalla 2.5K IPS
       - 16GB RAM
       - RTX 4050
       - Buena relaci√≥n calidad-precio
    """
    
    # Evaluar
    print("ü§ñ Evaluando recomendaciones con LLM Judge...")
    result = judge.evaluate_recommendations(
        user_analysis=user_analysis,
        search_criteria=search_criteria,
        recommendations=recommendations,
        products_found=3
    )
    
    if result['success']:
        eval_data = result['evaluation']
        
        print("\n‚≠ê Resultados de la evaluaci√≥n:")
        print(f"  Relevancia: {eval_data['relevancia']}/10")
        print(f"  Diversidad: {eval_data['diversidad']}/10")
        print(f"  Explicaci√≥n: {eval_data['explicacion']}/10")
        print(f"  Personalizaci√≥n: {eval_data['personalizacion']}/10")
        print(f"  Completitud: {eval_data['completitud']}/10")
        print(f"\n  üìä Score Total: {eval_data['score_total']}/10")
        print(f"  üèÜ Veredicto: {eval_data['veredicto']}")
        
        print(f"\nüí¨ Comentarios:")
        print(f"  {eval_data['comentarios']}")
        
        if eval_data.get('areas_mejora'):
            print(f"\nüîß √Åreas de mejora:")
            print(f"  {eval_data['areas_mejora']}")
    else:
        print(f"‚ùå Error en evaluaci√≥n: {result.get('error')}")
    
    return result


# ============================================================================
# EJEMPLO 5: Comparar rendimiento entre versiones
# ============================================================================

def example_5_compare_versions():
    """
    Compara resultados de evaluaciones de diferentes versiones
    √ötil para tracking de mejoras
    """
    print("\n" + "="*80)
    print("EJEMPLO 5: Comparaci√≥n de Versiones")
    print("="*80)
    
    import json
    from pathlib import Path
    
    # Buscar archivos de resultados
    results_dir = Path("evaluation/results")
    
    if not results_dir.exists():
        print("‚ö†Ô∏è  No hay resultados previos para comparar")
        return
    
    # Buscar archivos orchestrator (los m√°s completos)
    orchestrator_files = sorted(results_dir.glob("orchestrator_*.json"))
    
    if len(orchestrator_files) < 2:
        print(f"‚ö†Ô∏è  Solo hay {len(orchestrator_files)} archivo(s). Se necesitan al menos 2 para comparar.")
        return
    
    # Comparar los 2 m√°s recientes
    old_file = orchestrator_files[-2]
    new_file = orchestrator_files[-1]
    
    print(f"üìä Comparando:")
    print(f"  Anterior: {old_file.name}")
    print(f"  Actual: {new_file.name}")
    
    # Cargar datos
    with open(old_file, 'r', encoding='utf-8') as f:
        old_data = json.load(f)
    
    with open(new_file, 'r', encoding='utf-8') as f:
        new_data = json.load(f)
    
    # Comparar m√©tricas clave
    print("\nüìà Comparaci√≥n de m√©tricas:")
    
    # Tasa de √©xito
    old_rate = old_data['success_rate'] * 100
    new_rate = new_data['success_rate'] * 100
    diff_rate = new_rate - old_rate
    print(f"\n  Tasa de √©xito:")
    print(f"    Anterior: {old_rate:.1f}%")
    print(f"    Actual: {new_rate:.1f}%")
    print(f"    Cambio: {diff_rate:+.1f}% {'üìà' if diff_rate > 0 else 'üìâ' if diff_rate < 0 else '‚û°Ô∏è'}")
    
    # Tiempo promedio
    old_time = old_data['metrics']['execution_time']['avg']
    new_time = new_data['metrics']['execution_time']['avg']
    diff_time = new_time - old_time
    print(f"\n  Tiempo promedio:")
    print(f"    Anterior: {old_time:.2f}s")
    print(f"    Actual: {new_time:.2f}s")
    print(f"    Cambio: {diff_time:+.2f}s {'‚ö° M√°s r√°pido' if diff_time < 0 else 'üêå M√°s lento' if diff_time > 0 else '‚û°Ô∏è Igual'}")
    
    # Calidad LLM
    old_llm = old_data['metrics']['llm_judge_scores']['avg']
    new_llm = new_data['metrics']['llm_judge_scores']['avg']
    diff_llm = new_llm - old_llm
    print(f"\n  Calidad LLM Judge:")
    print(f"    Anterior: {old_llm:.1f}/10")
    print(f"    Actual: {new_llm:.1f}/10")
    print(f"    Cambio: {diff_llm:+.1f} {'‚≠ê Mejor' if diff_llm > 0 else '‚ö†Ô∏è Peor' if diff_llm < 0 else '‚û°Ô∏è Igual'}")
    
    # Veredicto final
    print(f"\nüéØ Veredicto:")
    if diff_rate > 5 and diff_time < 0 and diff_llm > 0:
        print("  üåü ¬°Mejora significativa en todos los aspectos!")
    elif diff_rate > 0 or diff_llm > 0:
        print("  ‚úÖ Mejoras detectadas")
    elif diff_rate < -5 or diff_llm < -1:
        print("  ‚ö†Ô∏è  Regresi√≥n detectada - revisar cambios")
    else:
        print("  ‚û°Ô∏è  Rendimiento similar")


# ============================================================================
# MAIN: Ejecutar ejemplos
# ============================================================================

def main():
    """Ejecuta todos los ejemplos"""
    print("\n" + "="*80)
    print("üß™ EJEMPLOS PR√ÅCTICOS - SISTEMA DE EVALUACI√ìN AURA")
    print("="*80)
    
    ejemplos = [
        ("1", "QuestionerAgent - Escenario Individual", example_1_questioner_single_scenario),
        ("2", "RAG - B√∫squeda Custom", example_2_rag_custom_query),
        ("3", "End-to-End - Escenario Custom", example_3_e2e_custom_scenario),
        ("4", "LLM Judge - Uso Directo", example_4_llm_judge_direct),
        ("5", "Comparaci√≥n de Versiones", example_5_compare_versions)
    ]
    
    print("\nEjemplos disponibles:")
    for num, desc, _ in ejemplos:
        print(f"  {num}. {desc}")
    
    print("\n" + "="*80)
    
    # Opci√≥n: ejecutar ejemplo espec√≠fico o todos
    import sys
    
    if len(sys.argv) > 1:
        ejemplo_num = sys.argv[1]
        for num, desc, func in ejemplos:
            if num == ejemplo_num:
                func()
                break
    else:
        # Ejecutar todos (comentar los que no quieras ejecutar)
        print("\n‚ö†Ô∏è  Por defecto, ejecuta EJEMPLO 4 (m√°s r√°pido)")
        print("Para ejecutar otro: python examples.py [1-5]\n")
        example_4_llm_judge_direct()


if __name__ == "__main__":
    main()

