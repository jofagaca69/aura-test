"""
Evaluador LLM-as-Judge para evaluar la calidad de las recomendaciones
"""
import json
import time
from typing import Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

from src.config import config
from evaluation.config import LLM_JUDGE_CONFIG


class LLMJudge:
    """
    Evaluador basado en LLM que juzga la calidad de las recomendaciones
    Utiliza Gemini para evaluaci칩n consistente y objetiva
    """
    
    def __init__(self):
        # Configurar timeout si est치 disponible en la versi칩n de langchain
        llm_kwargs = {
            "model": LLM_JUDGE_CONFIG["model"],
            "temperature": LLM_JUDGE_CONFIG["temperature"],
            "google_api_key": config.GOOGLE_API_KEY
        }
        
        # Intentar agregar timeout si est치 disponible
        timeout = LLM_JUDGE_CONFIG.get("timeout", 30.0)
        try:
            # Algunas versiones de langchain-google-genai soportan timeout
            if hasattr(ChatGoogleGenerativeAI, '__init__'):
                # Verificar si acepta timeout como par치metro
                import inspect
                sig = inspect.signature(ChatGoogleGenerativeAI.__init__)
                if 'timeout' in sig.parameters:
                    llm_kwargs['timeout'] = timeout
        except Exception:
            pass
        
        self.llm = ChatGoogleGenerativeAI(**llm_kwargs)
        self.timeout = timeout
        self.max_retries = LLM_JUDGE_CONFIG.get("max_retries", 3)
        
        # Prompt para evaluaci칩n de recomendaciones
        self.evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un evaluador experto de sistemas de recomendaci칩n de productos.
            Tu tarea es evaluar la calidad de las recomendaciones generadas por un asistente de IA.
            
            游늵 CRITERIOS DE EVALUACI칍N (escala 0-10):
            
            1. **RELEVANCIA** (0-10):
               - 쯃os productos recomendados coinciden con las necesidades del usuario?
               - 쯉e respetan las restricciones de presupuesto?
               - 쯃as caracter칤sticas solicitadas est치n presentes?
               
            2. **DIVERSIDAD** (0-10):
               - 쮿ay variedad apropiada en las opciones presentadas?
               - 쯉e ofrecen diferentes rangos de precio dentro del presupuesto?
               - 쯉e consideran diferentes marcas o alternativas?
               
            3. **EXPLICACI칍N** (0-10):
               - 쯃as justificaciones son claras y comprensibles?
               - 쯉e explica POR QU칄 cada producto es adecuado?
               - 쯉e mencionan pros y contras relevantes?
               
            4. **PERSONALIZACI칍N** (0-10):
               - 쯃as recomendaciones se adaptaron al contexto del usuario?
               - 쯉e consider칩 el uso espec칤fico mencionado?
               - 쯉e reflejan las preferencias expresadas?
               
            5. **COMPLETITUD** (0-10):
               - 쯉e abordaron todos los criterios mencionados por el usuario?
               - 쯉e proporcion칩 informaci칩n suficiente para tomar una decisi칩n?
               - 쯉e incluyeron detalles t칠cnicos relevantes?
            
            游닇 CONTEXTO DE LA EVALUACI칍N:
            
            **An치lisis del usuario:**
            {user_analysis}
            
            **Criterios de b칰squeda aplicados:**
            {search_criteria}
            
            **Recomendaciones generadas:**
            {recommendations}
            
            **N칰mero de productos encontrados:**
            {products_found}
            
            游꿢 INSTRUCCIONES:
            1. Eval칰a cada criterio objetivamente
            2. Proporciona un score num칠rico (0-10) para cada uno
            3. Calcula el score total como promedio de los 5 criterios
            4. Proporciona comentarios espec칤ficos y constructivos
            5. Sugiere mejoras concretas si aplica
            
            丘멆잺 S칄 ESTRICTO PERO JUSTO:
            - Un 10 es excepcional, raramente otorgado
            - Un 7-8 es bueno/muy bueno
            - Un 5-6 es aceptable pero mejorable
            - Menos de 5 indica problemas serios
            
            游늶 RESPONDE EN FORMATO JSON V츼LIDO (sin markdown, sin comentarios):
            {{
                "relevancia": <n칰mero 0-10>,
                "diversidad": <n칰mero 0-10>,
                "explicacion": <n칰mero 0-10>,
                "personalizacion": <n칰mero 0-10>,
                "completitud": <n칰mero 0-10>,
                "score_total": <promedio de los 5 scores>,
                "comentarios": "<feedback espec칤fico sobre qu칠 funcion칩 bien>",
                "areas_mejora": "<qu칠 se podr칤a mejorar>",
                "sugerencias": "<recomendaciones concretas para mejorar>",
                "veredicto": "<EXCELENTE|MUY_BUENO|BUENO|ACEPTABLE|DEFICIENTE>"
            }}"""),
            ("user", "Eval칰a estas recomendaciones:")
        ])
        
        # Prompt para evaluar calidad de preguntas
        self.question_evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un evaluador experto de conversaciones de venta consultiva.
            Tu tarea es evaluar la calidad de las preguntas que hace un asistente de IA.
            
            游늵 CRITERIOS DE EVALUACI칍N (escala 0-10):
            
            1. **CONTEXTUALIDAD** (0-10):
               - 쯃as preguntas se basan en respuestas previas?
               - 쮿ay conexi칩n l칩gica entre preguntas?
               - 쯉e evita preguntar lo mismo dos veces?
               
            2. **RELEVANCIA** (0-10):
               - 쯃as preguntas son pertinentes para la b칰squeda de productos?
               - 쮸yudan a entender mejor las necesidades del usuario?
               - 쯉on espec칤ficas y enfocadas?
               
            3. **NATURALIDAD** (0-10):
               - 쯉uenan conversacionales y humanas?
               - 쮼vitan ser rob칩ticas o formularias?
               - 쯊ienen el tono apropiado?
               
            4. **EFICIENCIA** (0-10):
               - 쯉e obtiene m치xima informaci칩n con m칤nimas preguntas?
               - 쯉on claras y f치ciles de responder?
               - 쯅o son redundantes?
               
            5. **COMPLETITUD** (0-10):
               - 쮺ubren todos los aspectos necesarios?
               - 쯇ermiten recopilar informaci칩n cr칤tica?
               - 쯉iguen una secuencia l칩gica?
            
            游닇 CONVERSACI칍N A EVALUAR:
            {conversation_history}
            
            游늵 INFORMACI칍N EXTRA칈DA:
            {extracted_info}
            
            游꿢 RESPONDE EN FORMATO JSON V츼LIDO:
            {{
                "contextualidad": <n칰mero 0-10>,
                "relevancia": <n칰mero 0-10>,
                "naturalidad": <n칰mero 0-10>,
                "eficiencia": <n칰mero 0-10>,
                "completitud": <n칰mero 0-10>,
                "score_total": <promedio>,
                "comentarios": "<an치lisis espec칤fico>",
                "mejores_preguntas": ["<ejemplos de preguntas bien hechas>"],
                "preguntas_mejorables": ["<ejemplos de preguntas que se podr칤an mejorar>"],
                "sugerencias": "<c칩mo mejorar>"
            }}"""),
            ("user", "Eval칰a la calidad de estas preguntas:")
        ])
    
    def _invoke_with_timeout(self, chain, inputs: Dict[str, Any]) -> Any:
        """Invoca la cadena con timeout y retry usando threading"""
        import threading
        
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                result_container = [None]
                exception_container = [None]
                
                def invoke_chain():
                    """Funci칩n que ejecuta la invocaci칩n en un thread separado"""
                    try:
                        result_container[0] = chain.invoke(inputs)
                    except Exception as e:
                        exception_container[0] = e
                
                # Crear thread para ejecutar la invocaci칩n
                thread = threading.Thread(target=invoke_chain, daemon=True)
                thread.start()
                thread.join(timeout=self.timeout)
                
                # Verificar si el thread a칰n est치 corriendo (timeout)
                if thread.is_alive():
                    raise TimeoutError(
                        f"La operaci칩n excedi칩 el timeout de {self.timeout}s "
                        f"(intento {attempt + 1}/{self.max_retries})"
                    )
                
                # Si hubo una excepci칩n en el thread, relanzarla
                if exception_container[0]:
                    raise exception_container[0]
                
                # Si tenemos resultado, retornarlo
                if result_container[0] is not None:
                    return result_container[0]
                
                # Si llegamos aqu칤, algo sali칩 mal
                raise Exception("No se obtuvo resultado ni excepci칩n")
                
            except TimeoutError as e:
                last_exception = e
                print(f"丘멆잺 Timeout en intento {attempt + 1}/{self.max_retries}: {e}")
                if attempt < self.max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # Backoff exponencial, m치ximo 5 segundos
                    time.sleep(wait_time)
                continue
            except Exception as e:
                last_exception = e
                error_str = str(e).lower()
                # Si es un error de red o API, reintentar
                if any(keyword in error_str for keyword in ["timeout", "connection", "network", "api"]):
                    print(f"丘멆잺 Error de conexi칩n en intento {attempt + 1}/{self.max_retries}: {e}")
                    if attempt < self.max_retries - 1:
                        wait_time = min(2 ** attempt, 5)  # Backoff exponencial
                        time.sleep(wait_time)
                    continue
                else:
                    # Otros errores no se reintentan
                    raise
        
        # Si todos los intentos fallaron
        raise last_exception or TimeoutError(
            f"Todos los {self.max_retries} intentos fallaron despu칠s de {self.timeout}s cada uno"
        )
    
    def evaluate_recommendations(
        self, 
        user_analysis: str,
        search_criteria: str,
        recommendations: str,
        products_found: int
    ) -> Dict[str, Any]:
        """
        Eval칰a la calidad de las recomendaciones usando Gemini
        
        Args:
            user_analysis: An치lisis de las necesidades del usuario
            search_criteria: Criterios de b칰squeda aplicados
            recommendations: Texto de recomendaciones generadas
            products_found: N칰mero de productos encontrados
            
        Returns:
            Diccionario con scores y feedback
        """
        try:
            chain = self.evaluation_prompt | self.llm
            
            inputs = {
                "user_analysis": user_analysis or "No disponible",
                "search_criteria": search_criteria or "No disponible",
                "recommendations": recommendations or "No se generaron recomendaciones",
                "products_found": products_found
            }
            
            # Truncar inputs muy largos para evitar timeouts
            max_length = 2000
            for key in ["user_analysis", "search_criteria", "recommendations"]:
                if key in inputs and len(str(inputs[key])) > max_length:
                    inputs[key] = str(inputs[key])[:max_length] + "... [truncado]"
            
            result = self._invoke_with_timeout(chain, inputs)
            
            # Parsear JSON
            response_text = result.content.strip()
            
            # Limpiar markdown si est치 presente
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation = json.loads(response_text)
            
            return {
                "success": True,
                "evaluation": evaluation,
                "raw_response": result.content
            }
            
        except (TimeoutError, Exception) as e:
            error_msg = str(e)
            print(f"丘멆잺 Error evaluando recomendaciones: {error_msg}")
            
            # Retornar evaluaci칩n por defecto en caso de timeout
            if "timeout" in error_msg.lower():
                return {
                    "success": False,
                    "error": "timeout",
                    "error_message": f"Timeout despu칠s de {self.timeout}s y {self.max_retries} intentos",
                    "evaluation": {
                        "relevancia": 5.0,
                        "diversidad": 5.0,
                        "explicacion": 5.0,
                        "personalizacion": 5.0,
                        "completitud": 5.0,
                        "score_total": 5.0,
                        "comentarios": "Evaluaci칩n no disponible debido a timeout",
                        "areas_mejora": "Timeout en evaluaci칩n LLM",
                        "sugerencias": "Revisar configuraci칩n de timeout o conexi칩n",
                        "veredicto": "NO_DISPONIBLE"
                    }
                }
            
            return {
                "success": False,
                "error": error_msg
            }
    
    def evaluate_questions(
        self,
        conversation_history: str,
        extracted_info: str
    ) -> Dict[str, Any]:
        """
        Eval칰a la calidad de las preguntas generadas por el QuestionerAgent
        
        Args:
            conversation_history: Historial completo de preguntas y respuestas
            extracted_info: Informaci칩n extra칤da de las respuestas
            
        Returns:
            Diccionario con scores y feedback
        """
        try:
            chain = self.question_evaluation_prompt | self.llm
            
            inputs = {
                "conversation_history": conversation_history or "No hay conversaci칩n",
                "extracted_info": extracted_info or "No se extrajo informaci칩n"
            }
            
            # Truncar inputs muy largos
            max_length = 2000
            for key in inputs:
                if len(str(inputs[key])) > max_length:
                    inputs[key] = str(inputs[key])[:max_length] + "... [truncado]"
            
            result = self._invoke_with_timeout(chain, inputs)
            
            # Parsear JSON
            response_text = result.content.strip()
            
            # Limpiar markdown
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation = json.loads(response_text)
            
            return {
                "success": True,
                "evaluation": evaluation,
                "raw_response": result.content
            }
            
        except json.JSONDecodeError as e:
            print(f"丘멆잺 Error parseando JSON: {e}")
            return {
                "success": False,
                "error": "JSON parsing error",
                "raw_response": response_text if 'response_text' in locals() else ""
            }
        except (TimeoutError, Exception) as e:
            error_msg = str(e)
            print(f"丘멆잺 Error evaluando preguntas: {error_msg}")
            
            # Retornar evaluaci칩n por defecto en caso de timeout
            if "timeout" in error_msg.lower():
                return {
                    "success": False,
                    "error": "timeout",
                    "error_message": f"Timeout despu칠s de {self.timeout}s y {self.max_retries} intentos",
                    "evaluation": {
                        "contextualidad": 5.0,
                        "relevancia": 5.0,
                        "naturalidad": 5.0,
                        "eficiencia": 5.0,
                        "completitud": 5.0,
                        "score_total": 5.0,
                        "comentarios": "Evaluaci칩n no disponible debido a timeout",
                        "mejores_preguntas": [],
                        "preguntas_mejorables": [],
                        "sugerencias": "Revisar configuraci칩n de timeout o conexi칩n"
                    }
                }
            
            return {
                "success": False,
                "error": error_msg
            }
    
    def evaluate_information_extraction(
        self,
        conversation: list,
        extracted_info: Dict[str, Any],
        expected_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Eval칰a la precisi칩n de la extracci칩n de informaci칩n
        
        Args:
            conversation: Lista de intercambios [pregunta, respuesta]
            extracted_info: Informaci칩n extra칤da por el sistema
            expected_info: Informaci칩n esperada (ground truth)
            
        Returns:
            M칠tricas de precisi칩n
        """
        scores = {}
        total_score = 0
        fields_evaluated = 0
        
        # Evaluar cada campo
        for field, expected_value in expected_info.items():
            extracted_value = extracted_info.get(field)
            
            if expected_value is None:
                # Si no se esperaba valor, verificar que no se extrajo nada incorrecto
                scores[field] = 1.0 if extracted_value is None or extracted_value == [] else 0.5
            elif isinstance(expected_value, list):
                # Para listas, calcular overlap
                if not expected_value:  # Lista vac칤a esperada
                    scores[field] = 1.0 if not extracted_value else 0.5
                else:
                    extracted_set = set(str(v).lower() for v in (extracted_value or []))
                    expected_set = set(str(v).lower() for v in expected_value)
                    
                    if not expected_set:
                        scores[field] = 1.0
                    else:
                        # Jaccard similarity
                        intersection = len(extracted_set & expected_set)
                        union = len(extracted_set | expected_set)
                        scores[field] = intersection / union if union > 0 else 0.0
            elif isinstance(expected_value, (int, float)):
                # Para n칰meros, verificar si est치 dentro del rango razonable
                if extracted_value is None:
                    scores[field] = 0.0
                else:
                    diff = abs(extracted_value - expected_value)
                    # Tolerancia del 10%
                    tolerance = expected_value * 0.1
                    scores[field] = 1.0 if diff <= tolerance else max(0, 1 - (diff / expected_value))
            else:
                # Para strings, verificar similitud sem치ntica simple
                if extracted_value is None:
                    scores[field] = 0.0
                else:
                    extracted_lower = str(extracted_value).lower()
                    expected_lower = str(expected_value).lower()
                    
                    # Similitud b치sica: palabras en com칰n
                    extracted_words = set(extracted_lower.split())
                    expected_words = set(expected_lower.split())
                    
                    if not expected_words:
                        scores[field] = 1.0
                    else:
                        overlap = len(extracted_words & expected_words)
                        scores[field] = overlap / len(expected_words)
            
            total_score += scores[field]
            fields_evaluated += 1
        
        # Calcular score promedio
        avg_score = total_score / fields_evaluated if fields_evaluated > 0 else 0.0
        
        return {
            "individual_scores": scores,
            "average_score": avg_score,
            "accuracy_percentage": avg_score * 100,
            "fields_evaluated": fields_evaluated,
            "perfect_matches": sum(1 for s in scores.values() if s >= 0.95),
            "partial_matches": sum(1 for s in scores.values() if 0.5 <= s < 0.95),
            "mismatches": sum(1 for s in scores.values() if s < 0.5)
        }

