"""
Evaluador LLM-as-Judge para evaluar la calidad de las recomendaciones
"""
import json
from typing import Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

from src.config import config
from evaluation.config import LLM_JUDGE_CONFIG


class LLMJudge:
    """
    Evaluador basado en LLM que juzga la calidad de las recomendaciones
    Utiliza Gemini para evaluaci칩n consistente y objetiva
    """
    
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model=LLM_JUDGE_CONFIG["model"],
            temperature=LLM_JUDGE_CONFIG["temperature"],
            google_api_key=config.GOOGLE_API_KEY
        )
        
        # Prompt para evaluaci칩n de recomendaciones
        self.evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un evaluador experto de sistemas de recomendaci칩n de productos.
            Tu tarea es evaluar la calidad de las recomendaciones generadas por un asistente de IA.
            
            游늵 CRITERIOS DE EVALUACI칍N (escala 0-10):
            
            1. **RELEVANCIA** (0-10):
               - 쯃os productos recomendados coinciden con las necesidades del usuario?
               - 쯉e respetan las restricciones de presupuesto?
               - 쯃as caracter칤sticas solicitadas est치n presentes?
               
            2. **DIVERSIDAD** (0-10):
               - 쮿ay variedad apropiada en las opciones presentadas?
               - 쯉e ofrecen diferentes rangos de precio dentro del presupuesto?
               - 쯉e consideran diferentes marcas o alternativas?
               
            3. **EXPLICACI칍N** (0-10):
               - 쯃as justificaciones son claras y comprensibles?
               - 쯉e explica POR QU칄 cada producto es adecuado?
               - 쯉e mencionan pros y contras relevantes?
               
            4. **PERSONALIZACI칍N** (0-10):
               - 쯃as recomendaciones se adaptaron al contexto del usuario?
               - 쯉e consider칩 el uso espec칤fico mencionado?
               - 쯉e reflejan las preferencias expresadas?
               
            5. **COMPLETITUD** (0-10):
               - 쯉e abordaron todos los criterios mencionados por el usuario?
               - 쯉e proporcion칩 informaci칩n suficiente para tomar una decisi칩n?
               - 쯉e incluyeron detalles t칠cnicos relevantes?
            
            游닇 CONTEXTO DE LA EVALUACI칍N:
            
            **An치lisis del usuario:**
            {user_analysis}
            
            **Criterios de b칰squeda aplicados:**
            {search_criteria}
            
            **Recomendaciones generadas:**
            {recommendations}
            
            **N칰mero de productos encontrados:**
            {products_found}
            
            游꿢 INSTRUCCIONES:
            1. Eval칰a cada criterio objetivamente
            2. Proporciona un score num칠rico (0-10) para cada uno
            3. Calcula el score total como promedio de los 5 criterios
            4. Proporciona comentarios espec칤ficos y constructivos
            5. Sugiere mejoras concretas si aplica
            
            丘멆잺 S칄 ESTRICTO PERO JUSTO:
            - Un 10 es excepcional, raramente otorgado
            - Un 7-8 es bueno/muy bueno
            - Un 5-6 es aceptable pero mejorable
            - Menos de 5 indica problemas serios
            
            游늶 RESPONDE EN FORMATO JSON V츼LIDO (sin markdown, sin comentarios):
            {{
                "relevancia": <n칰mero 0-10>,
                "diversidad": <n칰mero 0-10>,
                "explicacion": <n칰mero 0-10>,
                "personalizacion": <n칰mero 0-10>,
                "completitud": <n칰mero 0-10>,
                "score_total": <promedio de los 5 scores>,
                "comentarios": "<feedback espec칤fico sobre qu칠 funcion칩 bien>",
                "areas_mejora": "<qu칠 se podr칤a mejorar>",
                "sugerencias": "<recomendaciones concretas para mejorar>",
                "veredicto": "<EXCELENTE|MUY_BUENO|BUENO|ACEPTABLE|DEFICIENTE>"
            }}"""),
            ("user", "Eval칰a estas recomendaciones:")
        ])
        
        # Prompt para evaluar calidad de preguntas
        self.question_evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un evaluador experto de conversaciones de venta consultiva.
            Tu tarea es evaluar la calidad de las preguntas que hace un asistente de IA.
            
            游늵 CRITERIOS DE EVALUACI칍N (escala 0-10):
            
            1. **CONTEXTUALIDAD** (0-10):
               - 쯃as preguntas se basan en respuestas previas?
               - 쮿ay conexi칩n l칩gica entre preguntas?
               - 쯉e evita preguntar lo mismo dos veces?
               
            2. **RELEVANCIA** (0-10):
               - 쯃as preguntas son pertinentes para la b칰squeda de productos?
               - 쮸yudan a entender mejor las necesidades del usuario?
               - 쯉on espec칤ficas y enfocadas?
               
            3. **NATURALIDAD** (0-10):
               - 쯉uenan conversacionales y humanas?
               - 쮼vitan ser rob칩ticas o formularias?
               - 쯊ienen el tono apropiado?
               
            4. **EFICIENCIA** (0-10):
               - 쯉e obtiene m치xima informaci칩n con m칤nimas preguntas?
               - 쯉on claras y f치ciles de responder?
               - 쯅o son redundantes?
               
            5. **COMPLETITUD** (0-10):
               - 쮺ubren todos los aspectos necesarios?
               - 쯇ermiten recopilar informaci칩n cr칤tica?
               - 쯉iguen una secuencia l칩gica?
            
            游닇 CONVERSACI칍N A EVALUAR:
            {conversation_history}
            
            游늵 INFORMACI칍N EXTRA칈DA:
            {extracted_info}
            
            游꿢 RESPONDE EN FORMATO JSON V츼LIDO:
            {{
                "contextualidad": <n칰mero 0-10>,
                "relevancia": <n칰mero 0-10>,
                "naturalidad": <n칰mero 0-10>,
                "eficiencia": <n칰mero 0-10>,
                "completitud": <n칰mero 0-10>,
                "score_total": <promedio>,
                "comentarios": "<an치lisis espec칤fico>",
                "mejores_preguntas": ["<ejemplos de preguntas bien hechas>"],
                "preguntas_mejorables": ["<ejemplos de preguntas que se podr칤an mejorar>"],
                "sugerencias": "<c칩mo mejorar>"
            }}"""),
            ("user", "Eval칰a la calidad de estas preguntas:")
        ])
    
    def evaluate_recommendations(
        self, 
        user_analysis: str,
        search_criteria: str,
        recommendations: str,
        products_found: int
    ) -> Dict[str, Any]:
        """
        Eval칰a la calidad de las recomendaciones usando Gemini
        
        Args:
            user_analysis: An치lisis de las necesidades del usuario
            search_criteria: Criterios de b칰squeda aplicados
            recommendations: Texto de recomendaciones generadas
            products_found: N칰mero de productos encontrados
            
        Returns:
            Diccionario con scores y feedback
        """
        try:
            chain = self.evaluation_prompt | self.llm
            
            result = chain.invoke({
                "user_analysis": user_analysis or "No disponible",
                "search_criteria": search_criteria or "No disponible",
                "recommendations": recommendations or "No se generaron recomendaciones",
                "products_found": products_found
            })
            
            # Parsear JSON
            response_text = result.content.strip()
            
            # Limpiar markdown si est치 presente
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation = json.loads(response_text)
            
            return {
                "success": True,
                "evaluation": evaluation,
                "raw_response": result.content
            }
            
        except json.JSONDecodeError as e:
            print(f"丘멆잺 Error parseando JSON: {e}")
            print(f"Respuesta: {response_text[:500]}")
            return {
                "success": False,
                "error": "JSON parsing error",
                "raw_response": response_text
            }
        except Exception as e:
            print(f"丘멆잺 Error evaluando recomendaciones: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def evaluate_questions(
        self,
        conversation_history: str,
        extracted_info: str
    ) -> Dict[str, Any]:
        """
        Eval칰a la calidad de las preguntas generadas por el QuestionerAgent
        
        Args:
            conversation_history: Historial completo de preguntas y respuestas
            extracted_info: Informaci칩n extra칤da de las respuestas
            
        Returns:
            Diccionario con scores y feedback
        """
        try:
            chain = self.question_evaluation_prompt | self.llm
            
            result = chain.invoke({
                "conversation_history": conversation_history or "No hay conversaci칩n",
                "extracted_info": extracted_info or "No se extrajo informaci칩n"
            })
            
            # Parsear JSON
            response_text = result.content.strip()
            
            # Limpiar markdown
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation = json.loads(response_text)
            
            return {
                "success": True,
                "evaluation": evaluation,
                "raw_response": result.content
            }
            
        except json.JSONDecodeError as e:
            print(f"丘멆잺 Error parseando JSON: {e}")
            return {
                "success": False,
                "error": "JSON parsing error",
                "raw_response": response_text
            }
        except Exception as e:
            print(f"丘멆잺 Error evaluando preguntas: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def evaluate_information_extraction(
        self,
        conversation: list,
        extracted_info: Dict[str, Any],
        expected_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Eval칰a la precisi칩n de la extracci칩n de informaci칩n
        
        Args:
            conversation: Lista de intercambios [pregunta, respuesta]
            extracted_info: Informaci칩n extra칤da por el sistema
            expected_info: Informaci칩n esperada (ground truth)
            
        Returns:
            M칠tricas de precisi칩n
        """
        scores = {}
        total_score = 0
        fields_evaluated = 0
        
        # Evaluar cada campo
        for field, expected_value in expected_info.items():
            extracted_value = extracted_info.get(field)
            
            if expected_value is None:
                # Si no se esperaba valor, verificar que no se extrajo nada incorrecto
                scores[field] = 1.0 if extracted_value is None or extracted_value == [] else 0.5
            elif isinstance(expected_value, list):
                # Para listas, calcular overlap
                if not expected_value:  # Lista vac칤a esperada
                    scores[field] = 1.0 if not extracted_value else 0.5
                else:
                    extracted_set = set(str(v).lower() for v in (extracted_value or []))
                    expected_set = set(str(v).lower() for v in expected_value)
                    
                    if not expected_set:
                        scores[field] = 1.0
                    else:
                        # Jaccard similarity
                        intersection = len(extracted_set & expected_set)
                        union = len(extracted_set | expected_set)
                        scores[field] = intersection / union if union > 0 else 0.0
            elif isinstance(expected_value, (int, float)):
                # Para n칰meros, verificar si est치 dentro del rango razonable
                if extracted_value is None:
                    scores[field] = 0.0
                else:
                    diff = abs(extracted_value - expected_value)
                    # Tolerancia del 10%
                    tolerance = expected_value * 0.1
                    scores[field] = 1.0 if diff <= tolerance else max(0, 1 - (diff / expected_value))
            else:
                # Para strings, verificar similitud sem치ntica simple
                if extracted_value is None:
                    scores[field] = 0.0
                else:
                    extracted_lower = str(extracted_value).lower()
                    expected_lower = str(expected_value).lower()
                    
                    # Similitud b치sica: palabras en com칰n
                    extracted_words = set(extracted_lower.split())
                    expected_words = set(expected_lower.split())
                    
                    if not expected_words:
                        scores[field] = 1.0
                    else:
                        overlap = len(extracted_words & expected_words)
                        scores[field] = overlap / len(expected_words)
            
            total_score += scores[field]
            fields_evaluated += 1
        
        # Calcular score promedio
        avg_score = total_score / fields_evaluated if fields_evaluated > 0 else 0.0
        
        return {
            "individual_scores": scores,
            "average_score": avg_score,
            "accuracy_percentage": avg_score * 100,
            "fields_evaluated": fields_evaluated,
            "perfect_matches": sum(1 for s in scores.values() if s >= 0.95),
            "partial_matches": sum(1 for s in scores.values() if 0.5 <= s < 0.95),
            "mismatches": sum(1 for s in scores.values() if s < 0.5)
        }

